{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 物体の座標を取得する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 方法案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元のシーンの3Dモデリングと組み合わせることでフロアのどの位置に物体があるかの座標を斜めから撮影した動画を元に計算します。  \n",
    "元のシーンの3Dモデリングと組み合わせることで、斜めから撮影した動画を元に物体の座標を計算することが可能です。  \n",
    "以下は、そのための一般的な手順です：\n",
    "\n",
    "1. **3Dモデリング**: シーンの3Dモデルを作成します。これには、フロア、壁、天井などの構造が含まれることが一般的です。\n",
    "2. **カメラキャリブレーション**: カメラの内部パラメータ（焦点距離、主点など）と外部パラメータ（位置、向き）を特定します。これにより、カメラの視点と3Dワールドの座標系との関係が確立されます。\n",
    "3. **オブジェクト検出**: 斜めから撮影した動画から物体を検出します。これには、画像認識や物体検出のアルゴリズムが使用されることが一般的です。\n",
    "4. **2Dから3Dへのマッピング**: 物体の2D画像座標を3Dワールド座標に変換します。これには、カメラキャリブレーションで得られたパラメータと3Dモデルが使用されます。\n",
    "5. **トラッキング**: 動画の各フレームで物体の位置を追跡し、時間に応じた座標を計算します。これには、カルマンフィルターなどのトラッキングアルゴリズムが使用されることがあります。\n",
    "\n",
    "このプロセスは、コンピュータビジョンと3Dモデリングの専門的な知識を必要とします。  \n",
    "OpenCVやPCL（Point Cloud Library）などのライブラリは、これらのタスクを実行するための多くのツールを提供しています。  \n",
    "最終的な精度は、カメラの品質、シーンの複雑さ、使用されるアルゴリズムと技術の選択など、多くの要因に依存します。  \n",
    "適切なキャリブレーションと精密な3Dモデリングが行われれば、非常に正確な結果を得ることが可能です。  \n",
    "物体の2D画像座標を3Dワールド座標に変換するプロセスは、カメラキャリブレーションで得られた内部および外部パラメータを使用します。  \n",
    "以下は、その具体的な手順です：  \n",
    "\n",
    "1. **カメラキャリブレーション**: カメラの内部パラメータ（焦点距離、主点など）と外部パラメータ（回転、並進）を特定します。これらは、カメラの投影行列を構成します。\n",
    "2. **物体の2D座標の特定**: 画像から物体の2D座標を特定します。これは、物体の特定の特徴点（例：角）に対応する画像上のピクセル座標であることが一般的です。\n",
    "3. **3D座標の推定**: 物体が接触しているフロアの3D平面方程式を使用して、2D座標から3D座標を推定します。このプロセスは、以下のように行います：\n",
    "   - フロアの3D平面方程式を特定します（例：\\( ax + by + cz + d = 0 \\)）。\n",
    "   - カメラの投影行列と2D座標を使用して、対応する3D光線を計算します。\n",
    "   - この3D光線とフロアの3D平面が交差する点を計算します。この交点は、物体の3Dワールド座標です。\n",
    "4. **結果の検証**: 得られた3D座標が現実的かどうかを検証します。例えば、座標がフロアの範囲内にあるか、他の物体と衝突していないかなどを確認します。\n",
    "\n",
    "このプロセスは、数学的には線形代数と幾何学の概念を使用します。プログラミングの観点からは、OpenCVなどのライブラリがこれらの計算を容易にする多くの関数を提供しています。  \n",
    "最終的な精度は、カメラキャリブレーションの精度、画像解像度、物体の検出精度など、多くの要素に依存します。適切なプロセスと検証を通じて、この変換を非常に正確に行うことが可能です。\n",
    "\n",
    "__カメラキャリブレーション__  \n",
    "カメラキャリブレーションは、カメラの内部パラメータ（焦点距離、主点など）と外部パラメータ（回転、並進）を特定するプロセスです。  \n",
    "通常、既知の3Dオブジェクト（例：チェスボード）を使用して複数の画像を撮影し、これらの画像からパラメータを計算します。  \n",
    "このコードは、calibration_imagesフォルダ内のチェスボード画像を使用してカメラキャリブレーションを行います。  \n",
    "チェスボードのサイズとマスのサイズは、実際のチェスボードに合わせて調整する必要があります。  \n",
    "このプロセスにより、カメラの内部パラメータ（camera_matrix）と歪み係数（distortion_coefficients）が得られます。  \n",
    "各画像に対する外部パラメータ（回転と並進）も計算されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3D座標変換__  \n",
    "3D座標変換のプロセスで使用される方法は、幾何学と線形代数の基本的な原則に基づいています。\n",
    "以下は、このプロセスの各ステップと、なぜそれが物体の3D座標の計算に使用できるのかの説明です。\n",
    "1. **投影行列の計算**: 投影行列は、3D空間上の点を2D画像平面上の点に変換するために使用されます。\n",
    "この行列は、カメラの内部パラメータ（焦点距離、主点など）と外部パラメータ（回転、並進）から計算されます。\n",
    "2. **光線方向の計算**: 物体の2D画像座標から、カメラの中心を通る3D空間上の光線を計算します。\n",
    "この光線は、物体の実際の3D位置を通ります。投影行列の逆行列を使用して、この光線の方向を計算します。\n",
    "3. **光線の始点の計算**: 光線の始点はカメラの光学中心で、外部パラメータから計算されます。\n",
    "4. **光線と平面の交点の計算**: 物体が接触しているフロアは、3D空間上の平面として表現できます。\n",
    "この平面の方程式と、上記で計算した光線を使用して、光線と平面の交点を計算します。この交点は、物体の3Dワールド座標です。  \n",
    "\n",
    "この方法は、カメラの内部および外部パラメータが正確であると仮定しています。  \n",
    "また、物体が接触している平面（この場合、フロア）が既知である必要があります。  \n",
    "このプロセスは、コンピュータビジョンとロボティクスの分野で一般的に使用される方法で、3D空間上の物体の位置を特定するために使用されます。  \n",
    "カメラキャリブレーションによって得られた正確なパラメータを使用することで、2D画像から3D空間上の位置を信頼性高く推定することができます。  \n",
    "\n",
    "__フロア平面方程式__  \n",
    "物体が接触している平面（例：フロア）が既知であるという前提は、通常、システムの設計やセットアップの段階で確立される情報です。  \n",
    "以下は、この前提がどのように成立するかについての一般的なシナリオです。  \n",
    "\n",
    "1. **平面の方程式が既知**: システムの設計者が平面の方程式を事前に知っている場合、例えばフロアが完全に水平であるとわかっている場合など、この情報は直接使用できます。  \n",
    "2. **センサーまたは追加の情報**: 平面の方程式が直接既知でない場合、追加のセンサー（例：レーザー距離計、深度カメラなど）や手動での測定を使用して、平面の方程式を計算することができます。  \n",
    "3. **3Dモデリングとシミュレーション**: システムがシミュレーションまたは既知の3Dモデル内で動作する場合、平面の方程式はモデルから直接取得できます。  \n",
    "4. **特定の制約**: 物体が常に特定の方向に向いている、または特定の方向に移動するなどの制約がある場合、これらの制約を使用して平面の方程式を推定することができます。\n",
    "\n",
    "この前提が成立しない場合、平面の方程式を推定するために追加の手段が必要になることがあります。  \n",
    "これには、画像から平面を検出するアルゴリズム、追加のセンサーのデータ、人間による手動の測定などが含まれる可能性があります。  \n",
    "したがって、物体が接触している平面が既知であるかどうかは、システムの具体的な応用とセットアップに依存します。  \n",
    "この情報が既知でない場合、それを特定するための追加の手段が必要になることがあります。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "# カメラキャリブレーション\n",
    "# チェスボードの設定\n",
    "chessboard_size = (6, 9)\n",
    "square_size = 25\n",
    "object_points = np.array(\n",
    "    [[j * square_size, i * square_size, 0] for i in range(\n",
    "        chessboard_size[1]) for j in range(chessboard_size[0])], \n",
    "    dtype=np.float32)\n",
    "object_points_list, image_points = [], []\n",
    "\n",
    "# キャリブレーション画像の読み込みとコーナー検出\n",
    "images = glob.glob('calibration_images/*.jpg')\n",
    "for image_path in images:\n",
    "    image = cv2.imread(image_path)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, chessboard_size, None)\n",
    "\n",
    "    if ret:\n",
    "        object_points_list.append(object_points)\n",
    "        image_points.append(corners)\n",
    "\n",
    "# カメラキャリブレーションの実行\n",
    "ret, camera_matrix, distortion_coefficients, rotation_vectors, translation_vectors = \\\n",
    " cv2.calibrateCamera(object_points_list, image_points, gray.shape[::-1], None, None)\n",
    "\n",
    "# YOLOの設定\n",
    "yolo_config_path = 'yolov3.cfg'\n",
    "yolo_weights_path = 'yolov3.weights'\n",
    "yolo_classes_path = 'yolov3.txt'\n",
    "net = cv2.dnn.readNet(yolo_weights_path, yolo_config_path)\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# クラス名の読み込み\n",
    "with open(yolo_classes_path, 'r') as file:\n",
    "    classes = [line.strip() for line in file.readlines()]\n",
    "\n",
    "# 物体の2D座標の特定\n",
    "image = cv2.imread('cart_image.jpg')\n",
    "height, width = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(\n",
    "    image, scalefactor=0.00392, size=(416, 416), mean=(0, 0, 0), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "outs = net.forward(output_layers)\n",
    "\n",
    "# YOLOの出力から物体の2D座標を取得\n",
    "for out in outs:\n",
    "    for detection in out:\n",
    "        scores = detection[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if class_id == classes.index('cart') and confidence > 0.5:\n",
    "            center_x = int(detection[0] * width)\n",
    "            center_y = int(detection[1] * height)\n",
    "\n",
    "            # 3D座標変換\n",
    "            rotation_matrix = cv2.Rodrigues(rotation_vectors[0])[0]  # 回転ベクトルから回転行列への変換\n",
    "            translation_vector = translation_vectors[0]             # 並進ベクトル\n",
    "            plane_normal = np.array([0, 0, 1])                       # フロアの法線（Z軸方向）\n",
    "            plane_d = 0                                              # フロアの平面方程式の定数項\n",
    "            image_point = np.array([center_x, center_y, 1])          # 物体の2D座標（同次座標）\n",
    "            projection_matrix = camera_matrix @ np.hstack(\n",
    "                (rotation_matrix, translation_vector.reshape(-1, 1))) # 投影行列の計算\n",
    "            ray_direction = np.linalg.inv(projection_matrix) @ image_point # 光線方向の計算\n",
    "            ray_origin = -np.linalg.inv(rotation_matrix) @ translation_vector # 光線の始点（カメラの中心）の計算\n",
    "            t = (-(plane_d + plane_normal @ ray_origin) /\n",
    "                (plane_normal @ ray_direction)) # 光線と平面の交点のパラメータtの計算\n",
    "            intersection_point = ray_origin + t * ray_direction     # 光線と平面の交点（物体の3Dワールド座標）の計算\n",
    "\n",
    "            print(f\"Cart's 3D World Coordinate: {intersection_point}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Dモデルを使用したカメラキャリブレーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "カメラのキャリブレーションは、チェスボードを使用せずに、物体のある建屋の3Dモデルを使用して行うことが可能です。  \n",
    "この方法は、特に建築物や大規模なシーンでのカメラキャリブレーションに役立つことがあります。  \n",
    "以下は、このプロセスの一般的なステップです。\n",
    "\n",
    "1. **3Dモデルの取得**: 建屋の3Dモデルを取得する必要があります。  \n",
    "   これは、CADデータ、点群データ、または他の3Dモデリングソフトウェアから取得できる場合があります。\n",
    "2. **対応点の選択**: カメラの画像と3Dモデルの間で対応する点を選択する必要があります。  \n",
    "   これらの点は、建物の隅や窓の端など、画像と3Dモデルの両方で識別可能な特徴的な場所であるべきです。\n",
    "3. **対応点の2D-3Dマッピング**: 選択した対応点を使用して、2D画像座標と3Dワールド座標の間のマッピングを作成します。\n",
    "4. **キャリブレーションの実行**: 上記のマッピングを使用して、カメラの内部パラメータ（焦点距離、主点など）と外部パラメータ（回転、並進）を計算します。  \n",
    "   これには、OpenCVのようなライブラリで提供される関数を使用できます。\n",
    "5. **誤差の評価**: キャリブレーションの精度を評価するために、再投影誤差などのメトリックを使用することが一般的です。\n",
    "\n",
    "この方法の利点は、特定の環境やシーンに対してカスタマイズできることです。  \n",
    "欠点は、対応点の選択が手動で行う必要がある場合が多く、プロセスがより複雑で時間がかかることがあることです。  \n",
    "最近では、自動化された方法も開発されており、深層学習や他のコンピュータビジョン技術を使用して、対応点の選択やマッピングプロセスを自動化することが可能です。\n",
    "\n",
    "したがって、チェスボードを使用せずに建屋の3Dモデルを使用したカメラキャリブレーションは、実現可能であり、特定の応用に適している場合があります。\n",
    "最近開発された自動化手法について説明します。これらの手法は、カメラキャリブレーションプロセスをより効率的かつ正確に行うために使用されます。  \n",
    "### 1. 深層学習に基づく対応点の自動検出\n",
    "深層学習モデルは、画像内の特徴的な点を自動的に検出し、3Dモデルとの対応点を識別することができます。  \n",
    "このプロセスは、人間の介入を最小限に抑え、対応点の選択を高速化します。\n",
    "### 2. SLAM（Simultaneous Localization and Mapping）\n",
    "SLAM技術は、ロボティクスやAR/VRなどの分野で一般的に使用されます。  \n",
    "カメラの位置と姿勢をリアルタイムで推定しながら、同時に環境の3Dマップを構築します。  \n",
    "これにより、カメラの内部および外部パラメータの自動キャリブレーションが可能になります。\n",
    "### 3. 3D-2D対応の最適化\n",
    "最適化アルゴリズムを使用して、3Dモデルと2D画像間の対応関係を自動的に調整し、再投影誤差を最小化します。  \n",
    "これにより、手動での調整が不要になり、キャリブレーションの精度が向上します。\n",
    "### 4. センサー融合\n",
    "カメラ以外のセンサー（例：IMU、GPS）とのデータ融合を使用して、カメラの位置と姿勢の推定を補完します。  \n",
    "これにより、キャリブレーションプロセスがより堅牢で正確になります。\n",
    "### 5. クラウドベースのキャリブレーション\n",
    "クラウドコンピューティングを使用して、大量のデータと複数のカメラを同時に処理し、キャリブレーションプロセスを自動化およびスケールアップします。\n",
    "### まとめ\n",
    "これらの自動化手法は、カメラキャリブレーションのプロセスを効率化し、人間の介入を減らし、精度を向上させるために使用されます。  \n",
    "特に、大規模なシーンや複雑な環境でのキャリブレーションにおいて、これらの手法は非常に価値があります。  \n",
    "最新の研究と産業界の動向に応じて、これらの手法はさらに進化し、多岐にわたる応用が期待されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、3Dモデルと2D画像間の対応関係を自動的に調整し、再投影誤差を最小化する最適化アルゴリズムに関連するいくつかのリソースです。\n",
    "\n",
    "1. **[Projective Reconstruction from Multiple Views with Minimization of 2D Reprojection Error](https://www.researchgate.net/publication/220659415_Projective_Reconstruction_from_Multiple_Views_with_Minimization_of_2D_Reprojection_Error)**:  \n",
    "   この研究では、2D再投影誤差の最小化による射影再構成の問題が考察されています。バンドル調整技術などが取り上げられています。\n",
    "2. **[Photometric Bundle Adjustment for Dense Multi-View 3D](https://hal.science/hal-00985811/document)**:  \n",
    "   この論文では、画像の生成モデルと観測された画像との再投影誤差を直接最小化する方法が提案されています。\n",
    "3. **[Constrained Multiple Planar Reconstruction for Automatic](https://www.mdpi.com/1424-8220/21/14/4643)**:  \n",
    "   この研究では、複数の平面から投影された画像特徴の再投影誤差を共同で最適化する方法が提案されており、最終的には再投影誤差を大幅に削減することができるとされています。\n",
    "4. **[Reliable camera pose and calibration from a small set of](https://www.sciencedirect.com/science/article/abs/pii/S1077314210002584)**:  \n",
    "   この研究では、ノイズのある2Dおよび3D特徴間の対応関係からカメラの姿勢と較正を解決する新しい方法が提示されています。\n",
    "\n",
    "これらのリソースは、3Dモデルと2D画像間の対応関係を自動的に調整し、再投影誤差を最小化する最適化アルゴリズムに関連する最新の研究と技術を提供しています。  \n",
    "特定のプロジェクトや応用に合わせて、これらの手法をさらに探求することが有益であるかもしれません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__camera pose and calibration__\n",
    "* [pose_estimation_and_geometric_camera_calibration](https://wiki.eecs.yorku.ca/course_archive/2018-19/F/4422/_media/6.2_pose_estimation_and_geometric_camera_calibration.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
